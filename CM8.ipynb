{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>ASSIGNMENT 1</center></h2>\n",
    "<h2><center>Name: DEEP ASHISH JARIWALA</center></h2>\n",
    "<h2><center>SID: 20909290</center></h2>\n",
    "<h2><center>Q: CM8</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Explain why you had to split the dataset into train, validation and test sets?</h3>\n",
    "<p>--> There is a significant difference between the test set and validation set in determining the performance of the machine learning model. The validation set is used to test the model in an unbiased way to obtain the best hyperparameters. Ideally, a machine learning algorithm must be tested on a dataset never seen by the model. The validation set also plays an essential role in feature selection, which cannot be performed on a test set as it could lead to inappropriate results of the final model. Therefore, testing on the test set by changing hyperparameters can lead to false results or overfitted models that ultimately perform weakly on an entirely new test set. To conclude, keep the test set aside and once the model is ready with the best features and tuned hyperparameters, test the model to get accurate results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Explain why you didn't evaluate directly on the test set and had\n",
    "to use a validation test when finding the best parameters for KNN?</h3>\n",
    "<p>--> There are many hyperparameters in the K-nearest neighbors algorithm when it comes to training the model. To set the value of k (it has many variations starting from 1 upto 40), distance metric p(it has variations like euclidean, Minkowski, manhattan), and weights(based on distance or uniform weight distribution) validation set are used. For instance, I have used (k = 5, p = 2, and weights = distance) for the iris dataset, which gave the best performance for the validation set. Then, using these parameters, test set gave an accurate performance. If the hyperparameter tuning were evaluated on the test set, it could mislead the performance as an overfitted model. Thus, changing hyperparamters using test set can lead to false accuracy and other perfomance metrics which would perform poorly on an entirely new dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What was the effect of changing k for KNN. Was the accuracy always affected the same way with an increase of k? Why do you think this happened?</h3>\n",
    "<p>-->The parameter k is used to determine the number of nearest neighbors used to calculate the probability of a test case in order to predict its classification. However, there is no fix value of k but we use the validation set to determine the value of k (fitting best in the training set). The general trend is that as we increase k error decreases till it reaches a minimum then it start to increase (indicating overfitting). The accuracy is affected at every change in k as the number of nearest neighbors increases that changes the probability of classification. In case of heart disease dataset, the accuracy improved as the value of k is increased uptill 20 then it remains constant and finally started decreasing.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
